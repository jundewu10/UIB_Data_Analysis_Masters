{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Scrapping de los Investigadores de la UIB\n",
    "## Jun De Wu\n",
    "\n",
    "El objetivo del trabajo es entrar en la página web de la UIB, en concreto en la sección de grupos de investigación. Dentro de esta página, recorrer por cada grupo extrayendo información de los investigadores como grupo de investigación, nombre, género, titulación, puesto y currículum vitae. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos por cargar las librerías que vamos a necesitar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos dos funciones que sirven para grabar y leer ficheros para no tener que volver a realizar todo el proceso cada vez que entramos a modificar algo del trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_data(data, filename):\n",
    "    with open(filename, \"w\") as out_file:\n",
    "        json.dump(data, out_file)\n",
    "        \n",
    "def load_data(filename):\n",
    "    data = None\n",
    "    with open(filename, \"r\") as in_file:\n",
    "        data = json.load(in_file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de entrar de lleno a explicar las funciones que emplearemos a lo largo del documento, vamos a explicar la idea general del trabajo. Nosotros empezamos por acceder a las páginas de la UIB donde están numeradas los grupos de investigación en 15 páginas diferentes. El enlace para la página número 1 es https://www.uib.cat/recerca/estructures/grups/grups_area/id_area=-1%2526npag=1 y podemos acceder a las otra cambiando el último 1 por la página que queremos acceder. Dentro de cada grupo tenemos que acceder al equipo de investigación y aquí es donde viene el problema. De forma general, tomando como ejemplo la página web del grupo \"Gènetica Humana\", las páginas web de los grupos vienen dado de la forma https://www.uib.cat/recerca/estructures/grups/grup/GENEHUMA/ + \"equip/index.html\". \n",
    "\n",
    "Ahora bien, después de mirar cada grupo de forma individual hemos captado que hay un total de 18 grupos de investigación que su página web no sigue esta estructura. A continuación detallamos los enlaces de estos grupos: \n",
    "\n",
    "* https://engalim.uib.cat/Grupo-investigador/\n",
    "* https://gicafe.uib.cat/Estructura_I_Personal/\n",
    "* https://eic.uib.cat/Equip-investigador/\n",
    "* https://gedhe.uib.cat/Equip-investigador/\n",
    "* https://gifes.uib.cat/Research-staff/\n",
    "* https://meteo.uib.cat/equip/\n",
    "* https://recumare.uib.cat/EquipInvestigador/\n",
    "* https://reacmole.uib.cat/Equip-investigador/\n",
    "* https://ccts.uib.cat/Equipo-investigador/\n",
    "* https://desigualtats.uib.cat/Equip-investigador/\n",
    "* https://gresib.uib.cat/Equip-investigador/\n",
    "* https://relatmit.uib.cat/Equip-investigador/\n",
    "* https://grupestudidha.uib.cat/Equip-investigador/\n",
    "* https://praxis.uib.cat/Equip/\n",
    "* https://imasdel.uib.cat/Equip/\n",
    "* https://ndpc.uib.cat/Equip-investigador/\n",
    "* https://acsic.uib.cat/EquipInv/\n",
    "* https://scopia.uib.cat/Equip/\n",
    "\n",
    "El primer enlace es un grupo sumamente raro, con lo cual lo quitaremos de nuestros grupos extraños. Guardaremos estos enlaces en una lista para tratarlos más tarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grupos_raros = [\"https://gicafe.uib.cat/Estructura_I_Personal/\",\n",
    "                       \"https://eic.uib.cat/Equip-investigador/\", \"https://gedhe.uib.cat/Equip-investigador/\",\n",
    "                       \"https://gifes.uib.cat/Research-staff/\",\"https://meteo.uib.cat/equip/\",\n",
    "                       \"https://recumare.uib.cat/EquipInvestigador/\", \"https://reacmole.uib.cat/Equip-investigador/\",\n",
    "                       \"https://ccts.uib.cat/Equipo-investigador/\", \"https://desigualtats.uib.cat/Equip-investigador/\",\n",
    "                       \"https://gresib.uib.cat/Equip-investigador/\", \"https://relatmit.uib.cat/Equip-investigador/\",\n",
    "                       \"https://grupestudidha.uib.cat/Equip-investigador/\", \"https://praxis.uib.cat/Equip/\",\n",
    "                       \"https://imasdel.uib.cat/Equip/\", \"https://ndpc.uib.cat/Equip-investigador/\",\n",
    "                       \"https://acsic.uib.cat/EquipInv/\", \"https://scopia.uib.cat/Equip/\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al entrar tantas veces por las mismas páginas de la UIB, la página web ha empezado a bloquear la entrada y no podemos scrappear la información de los investigadores. Añadimos el parámetro headers al utilizar la función `requests.get` para que nos detecte como un usuario y no un bot. La función `load_page` lee los enlaces url que le pasamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) '\\\n",
    "           'AppleWebKit/537.36 (KHTML, like Gecko) '\\\n",
    "           'Chrome/75.0.3770.80 Safari/537.36'}\n",
    "def load_page(url):\n",
    "    pagina_categorias = requests.get(url, headers = headers)\n",
    "    if pagina_categorias.status_code == 200:\n",
    "        return BeautifulSoup(pagina_categorias.text,'html5')\n",
    "    else:\n",
    "        print(\"Algo va mal\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `extrae_grupo` tiene como parámetro los enlaces de los grupos de investigación y nos devuelve el nombre del grupo en cuestión y el enlace a la página del equipo de investigación del grupo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrae_grupo(item):\n",
    "    nombre = item.text.strip()\n",
    "    url = item.find(\"a\").get(\"href\")\n",
    "    return nombre, url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tema de extraer el currículum vitae de los investigadores es un tema complicado. Nosotros estamos tratando en general con la página de la UIB en catalán, pero hay cv's que no están disponibles en catalán pero sí en castellano o en inglés. Lo que haremos será un recorrido en el siguiente orden: catalán-castellano-inglés. El procedimiento sería:\n",
    "\n",
    "* Buscar el cv en la página en catalán y si está finalizamos, si no pasamos al segundo punto.\n",
    "* Buscamos el cv en la página en castellano y si está finalizamos, si no pasamos al último punto.\n",
    "* Buscamos el cv en la página en inglés y si está finalizamos, si no devolvemos \"No hay cv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrae_cv(url):\n",
    "    pagina_cv = load_page(url)\n",
    "    if pagina_cv is not None:\n",
    "        if pagina_cv.find(\"div\", class_=\"uib_style_nolanguageversion\") is not None:\n",
    "            url_es = url.replace(\"cat\", \"es/es\")\n",
    "            pagina_cv_es = load_page(url_es)\n",
    "            if pagina_cv_es.find(\"div\", class_=\"uib_style_nolanguageversion\") is not None:\n",
    "                url_eu = url.replace(\"cat\", \"eu\")\n",
    "                pagina_cv_eu = load_page(url_eu)\n",
    "                if pagina_cv_eu.find(\"div\", class_=\"uib_style_nolanguageversion\") is not None:\n",
    "                    return \"No hay cv\"\n",
    "                if pagina_cv_eu.find(\"div\", id=\"cv_breve\") is not None:\n",
    "                    return pagina_cv_eu.find(\"div\", id=\"cv_breve\").text\n",
    "                else:\n",
    "                    return \"No hay cv\"\n",
    "            if pagina_cv_es.find(\"div\", id=\"cv_breve\") is not None:\n",
    "                return pagina_cv_es.find(\"div\", id=\"cv_breve\").text\n",
    "            else:\n",
    "                return \"No hay cv\"\n",
    "        if pagina_cv.find(\"div\", id=\"cv_breve\") is not None:\n",
    "            return pagina_cv.find(\"div\", id=\"cv_breve\").text\n",
    "        else: \n",
    "            return \"No hay cv\"\n",
    "    else:\n",
    "        return \"No hay cv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función principal que utilizaremos para extraer la información de los investigadores es `extra_investigador` con parámetros la url de la página del investigador, el tipo de investigador y el grupo al que pertenece. Utilizaremos dentro de esta función la función `extrae_cv` mencionada anteriormente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_investigador(item, tipo, grupo):\n",
    "    cv = None\n",
    "    #Si no existe la página del investigador devolvemos None\n",
    "    if item.find(\"a\") == None:\n",
    "        url = None  \n",
    "    #Si existe guardamos en url el enlace\n",
    "    else:    \n",
    "        url = item.find(\"a\").get(\"href\") \n",
    "    texto = item.text \n",
    "    partes = texto.split('(')\n",
    "    p1 = partes[0].strip().split()\n",
    "    nombre = \" \".join(p1[1:])\n",
    "    if p1[0][2]=='a': \n",
    "        genero = \"FEMENINO\"\n",
    "    else:\n",
    "        genero = \"MASCULINO\"\n",
    "    tipo_investigador = tipo\n",
    "    titulo = p1[0]\n",
    "    #Puede no tener especificado el puesto del investigador, si es así devolvemos \"No se sabe\"\n",
    "    if len(partes) == 2: \n",
    "        relacion_uib = partes[1][:-1]\n",
    "    else: \n",
    "        relacion_uib = \"No se sabe\"\n",
    "    #Si existe la página del investigador no es None extraemos su cv\n",
    "    if url is not None: \n",
    "        cv = extrae_cv(url)\n",
    "    else:\n",
    "        cv = \"No hay cv\"\n",
    "    #Devolvemos la información como un diccionario\n",
    "    return {\"grupo\" : grupo, \"nombre\" : nombre, \"genero\" : genero, \"tipo de investigador\" : tipo_investigador, \n",
    "            \"titulo\" : titulo, \"puesto\" : relacion_uib, \"cv\" : cv} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaramos una lista vacía llamada `investigadores` donde iremos guardando la información de los investigadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "investigadores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llegamos al cuerpo principal del trabajo. Consiste en una serie de bucles `for` para ir recorriendo las páginas de forma gradual y extraemos la información de los investigadores. La estructura del algoritmo se especifica de la siguiente forma:\n",
    "\n",
    "* El primer `for` es para ir recorriendo las 15 páginas que contienen los diferentes grupos de investigación que hay.\n",
    "* El segundo `for` lo utilizamos para entrar en cada grupo de investigación y en la página del equipo para conseguir la lista de los investigadores.\n",
    "* El tercer `for` sirve para guardar en un la variable `personal` la lista de los investigadores.\n",
    "* El cuarto y último `for` ya es el último paso, extraemos la información de los investigadores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo va mal\n",
      "Algo va mal\n",
      "Algo va mal\n",
      "Algo va mal\n",
      "Algo va mal\n",
      "Algo va mal\n",
      "Algo va mal\n",
      "Algo va mal\n",
      "Algo va mal\n",
      "Algo va mal\n",
      "Algo va mal\n",
      "Algo va mal\n",
      "Algo va mal\n",
      "Algo va mal\n",
      "Algo va mal\n",
      "Algo va mal\n",
      "Algo va mal\n",
      "Algo va mal\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,16):\n",
    "    #Leemos en cada iteración la página principal donde están los grupos de investigación\n",
    "    page = load_page(\"https://www.uib.cat/recerca/estructures/grups/grups_area/id_area=-1%2526npag=\" + str(i))\n",
    "    if page == None:\n",
    "        continue\n",
    "    #Guardajmos los grupos de investigación\n",
    "    contenedor = page.find(\"div\", class_=\"uib_style_filaunica\")\n",
    "    grupos = contenedor.find_all(\"li\")\n",
    "    for j in range(0, len(grupos)):\n",
    "        #Extraemos el nombre del grupo y el enlace a su equipo de investigación\n",
    "        a, b = extrae_grupo(grupos[j])\n",
    "        web_grupo = b + \"/equip/index.html\"\n",
    "        pgrupo = load_page(web_grupo)\n",
    "        #Si no existe el enlace y es uno de los 18 casos extraños mencionados anteriormente, seguimos con la siguiente iteración\n",
    "        if pgrupo == None:\n",
    "            continue\n",
    "        #Guardamos la lista de las categorías de investigadores\n",
    "        contenido = pgrupo.find(\"div\", itemprop=\"mainContentOfPage\")\n",
    "        lista_rel = contenido.find_all(\"h3\")\n",
    "        for k in range(0, len(lista_rel)):\n",
    "            #Guardamos los investigadores de cada categoría que estamos recorriendo\n",
    "            personal = lista_rel[k].find_next_sibling().find_all(\"li\")\n",
    "            for l in range(0, len(personal)):\n",
    "                #Extraemos la información de cada investigador\n",
    "                investigadores.append(extra_investigador(personal[l], lista_rel[k].text, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hemos extraído los datos de los investigadores de las páginas regulares es hora de extraerlas de las páginas raras que hemos guardado en la lista `grupos_raros`. Guardamos estos investigadores también en la lista `investigadores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(grupos_raros)):\n",
    "        pgrupo = load_page(grupos_raros[i])\n",
    "        if pgrupo == None:\n",
    "            continue\n",
    "        titulo = pgrupo.find(\"div\", id = \"menu-navegacio-seccio\")\n",
    "        a = titulo.find(\"a\").text\n",
    "        contenido = pgrupo.find(\"div\", itemprop=\"mainContentOfPage\")\n",
    "        lista_rel = contenido.find_all(\"h3\")\n",
    "        for k in range(0, len(lista_rel)):\n",
    "            personal = lista_rel[k].find_next_sibling().find_all(\"li\")\n",
    "            for l in range(0, len(personal)):\n",
    "                investigadores.append(extra_investigador(personal[l], lista_rel[k].text, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora utilizamos la función `dump_data` para grabar los datos en un fichero llamado también `investigadores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_data(investigadores, \"investigadores.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El trabajo estaría ya realizado pero si se quiere hacer comprobaciones de los datos obtenidos, siempre se puede cargar el fichero `investigadores` con la función `load_page`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
